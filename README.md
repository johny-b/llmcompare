# llmcompare

## Quickstart

TODO

## Examples

Examples 1-4 demonstrate all key functionalities of LLMCompare.

| # | Example | Description |
|---|---------|-------------|
| 1 | [free_form_question.py](examples/free_form_question.py) | Basic FreeForm question. |
| 2 | [next_token_question.py](examples/next_token_question.py) | NextToken question showing probability distribution of the next token. |
| 3 | [rating_question.py](examples/rating_question.py) | Rating question that extracts numeric scores from logprobs. |
| 4 | [judges.py](examples/judges.py) | FreeForm question with responses evaluated by judges. |
| 5 | [questions_in_yaml.py](examples/questions_in_yaml.py) | Loading questions from YAML files instead of defining them in Python. |
| 6 | [configuration.py](examples/configuration.py) | Using the Config class to configure llmcompare settings at runtime. |
| 7 | [tinker.py](examples/tinker.py) | Using Tinker models via OpenAI-compatible API. |
| 8 | [openrouter.py](examples/openrouter.py) | Using OpenRouter models via OpenAI-Compatible API. |
| 9 | [x_mod_57.py](examples/x_mod_57.py) | Full example testing model arithmetic capabilities (x modulo 57). |
| 10 | [runner.py](examples/runner.py) | Direct Runner usage for low-level API interactions. |

## Providers

TODO [OpenAI, Tinker, openrouter, only OpenAI tested]

### API keys

TODO
